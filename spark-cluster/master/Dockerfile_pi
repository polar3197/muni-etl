FROM --platform=linux/arm64 python:3.10-slim

# Install dependencies, download & extract Spark, cleanup
ENV SPARK_VERSION="3.5.6"
ENV JAVA_HOME="/usr/lib/jvm/java-17-openjdk-arm64"
ENV SPARK_HOME="/opt/spark"
ENV PYSPARK_PYTHON="python3"
ENV PATH="${JAVA_HOME}/bin:${SPARK_HOME}/sbin:${SPARK_HOME}/bin:$PATH"

ENV HADOOP_AWS_VERSION=3.3.2
ENV AWS_BUNDLE_VERSION=1.11.1026
ENV SPARK_JARS_DIR=$SPARK_HOME/jars

EXPOSE 7077 8080

# Install dependencies and Spark
RUN apt-get update && \
    apt-get install -y --no-install-recommends \
        openjdk-17-jre-headless \
        curl \
        wget \
        ca-certificates \
        gnupg \
        procps \
        iproute2 && \
    rm -rf /var/lib/apt/lists/* && \
    curl -L -o /tmp/spark.tgz https://dlcdn.apache.org/spark/spark-${SPARK_VERSION}/spark-${SPARK_VERSION}-bin-hadoop3.tgz && \
    tar -xzf /tmp/spark.tgz -C /opt && \
    mv /opt/spark-${SPARK_VERSION}-bin-hadoop3 ${SPARK_HOME} && \
    rm /tmp/spark.tgz

# Download AWS and Hadoop JARs into Spark
RUN curl -L -o ${SPARK_JARS_DIR}/hadoop-aws-${HADOOP_AWS_VERSION}.jar \
      https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-aws/${HADOOP_AWS_VERSION}/hadoop-aws-${HADOOP_AWS_VERSION}.jar && \
    curl -L -o ${SPARK_JARS_DIR}/aws-java-sdk-bundle-${AWS_BUNDLE_VERSION}.jar \
      https://repo1.maven.org/maven2/com/amazonaws/aws-java-sdk-bundle/${AWS_BUNDLE_VERSION}/aws-java-sdk-bundle-${AWS_BUNDLE_VERSION}.jar

# Install PySpark and data processing packages
RUN pip install --no-cache-dir pyspark pandas pyarrow requests

# Configuration scripts
COPY conf/spark-env.sh ${SPARK_HOME}/conf/spark-env.sh
COPY master/start-master.sh /opt/start-master.sh

RUN chmod +x ${SPARK_HOME}/conf/spark-env.sh /opt/start-master.sh

CMD ["/opt/start-master.sh"]